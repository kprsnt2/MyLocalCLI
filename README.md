# MyLocalCLI âš¡

**Your Own AI Coding Assistant - Private, Local, Yours**

![MyLocalCLI](https://img.shields.io/badge/MyLocalCLI-v2.0.0-purple)
![Node.js](https://img.shields.io/badge/Node.js-18%2B-green)
![License](https://img.shields.io/badge/License-MIT-blue)

## âœ¨ Features

### ğŸ¤– Multi-Provider Support
| Provider | Type | Description |
|----------|------|-------------|
| ğŸ  **LM Studio** | Local | Connect to your local LLM |
| ğŸ¦™ **Ollama** | Local | Another popular local option |
| ğŸŒ **OpenRouter** | Cloud | Free models (Llama 3.3, Gemma, etc.) |
| ğŸ”‘ **OpenAI** | Cloud | GPT-4o, GPT-4, etc. |
| âš¡ **Groq** | Cloud | Ultra-fast inference |
| âš™ï¸ **Custom** | Any | Any OpenAI-compatible API |

### ğŸ› ï¸ Tool Calling
- **Read/Write Files** - AI can read and modify files
- **Execute Commands** - Run shell commands safely
- **Git Integration** - Status, diff, and more

### ğŸ’¬ Conversation Management
- Save & load conversations
- Export to Markdown
- Named sessions

### ğŸŒ Web UI
- Beautiful dark theme
- Voice input (Chrome/Edge)
- Provider switching

## ğŸš€ Quick Start

```bash
# Install dependencies
npm install

# Setup wizard
node src/index.js init

# Start CLI chat
node src/index.js

# Start Web UI
node src/index.js web
```

After `npm link`, you can use:
```bash
mylocalcli      # CLI mode
mlc             # Short alias
mylocalcli web  # Web UI
```

## ğŸ“– Commands

### CLI
```bash
mylocalcli                    # Start chat
mylocalcli init               # Setup wizard
mylocalcli config --show      # View config
mylocalcli models             # List models
mylocalcli providers          # List providers
mylocalcli web                # Start web UI
mylocalcli history --list     # List conversations
```

### In-Chat Commands
| Command | Description |
|---------|-------------|
| `/help` | Show help |
| `/clear` | Clear conversation |
| `/models` | List models |
| `/exit` | Exit |

## ğŸ¦™ Ollama Setup

```bash
ollama pull llama3.2
ollama serve
mylocalcli config --provider ollama
```

## ğŸ  LM Studio Setup

1. Download [LM Studio](https://lmstudio.ai)
2. Load a model
3. Start the server (port 1234)
4. Run `mylocalcli init`

## ğŸŒ OpenRouter (Free Models)

1. Get API key at [openrouter.ai/keys](https://openrouter.ai/keys)
2. Run `mylocalcli init` and select OpenRouter

## ğŸ“ Project Structure

```
mylocalcli/
â”œâ”€â”€ package.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.js          # CLI entry
â”‚   â”œâ”€â”€ config/           # Settings
â”‚   â”œâ”€â”€ providers/        # LLM providers
â”‚   â”œâ”€â”€ core/             # Chat, tools
â”‚   â”œâ”€â”€ ui/               # Terminal UI
â”‚   â””â”€â”€ utils/            # Utilities
â””â”€â”€ web/
    â””â”€â”€ index.html        # Web UI
```

## ğŸ“„ License

MIT License

---

**MyLocalCLI** - Your Own AI Coding Assistant ğŸš€
